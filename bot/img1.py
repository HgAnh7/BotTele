import time
import random
import requests
from bs4 import BeautifulSoup
from telebot.types import InputFile

def get_all_image_urls():
    headers = {"User-Agent": "Mozilla/5.0"}
    base_url = "https://cosplaytele.com/category/byoru/"

    all_image_urls = []
    visited_albums = []
    albums = []  # Khai b√°o danh s√°ch ch·ª©a album URL

    try:
        response = requests.get(base_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")

        a_tags = soup.find_all("a", class_="page-number")
        if a_tags:
            last_page = a_tags[-2].text
            print(f"T·ªïng s·ªë trang: {last_page}")
        else:
            print("Kh√¥ng t√¨m th·∫•y s·ªë trang.")
            return

        for page in range(1, int(last_page) + 1):
            print(f"ƒêang x·ª≠ l√Ω trang {page}...")
            url_page = f"{base_url}page/{page}/"

            response = requests.get(url_page, headers=headers, timeout=10)
            soup = BeautifulSoup(response.text, "html.parser")  # C·∫≠p nh·∫≠t l·∫°i soup cho trang m·ªõi

            # T√¨m t·∫•t c·∫£ c√°c album li√™n k·∫øt
            for tag in soup.find_all(True):
                if tag.name == "a" and tag.has_attr("href") and "plain" in tag.get("class", []):
                    albums.append(tag['href'])

                # D·ª´ng khi g·∫∑p "Popular Cosplay"
                if tag.name == "span" and tag.get("class") == ["section-title-main"]:
                    if "Popular Cosplay" in tag.text:
                        break  # D·ª´ng l·∫°i khi g·∫∑p th·∫ª n√†y

            # Lo·∫°i b·ªè album tr√πng l·∫∑p
            unique_urls = list(set(albums))

            # Th√™m c√°c album v√†o danh s√°ch visited_albums
            for album_url in unique_urls:
                if album_url not in visited_albums:
                    visited_albums.append(album_url)

            print(f"ƒê√£ t√¨m th·∫•y {len(visited_albums)} album.")

        # L·∫•y ·∫£nh t·ª´ c√°c album
        for album_url in visited_albums:
            print(f"  ‚Üí L·∫•y ·∫£nh t·ª´ album: {album_url}")
            try:
                response = requests.get(album_url, headers=headers, timeout=10)
                soup = BeautifulSoup(response.text, "html.parser")

                img_tags = soup.find_all("img", src=True, class_="attachment-full size-full", alt=True)
                for img in img_tags:
                    src = img.get("src", "")
                    if src.startswith("https://"):
                        all_image_urls.append(src)

            except Exception as err:
                print(f"    ‚ö†Ô∏è L·ªói album: {err}")

            # Th√™m th·ªùi gian ch·ªù gi·ªØa c√°c request ƒë·ªÉ gi·∫£m t·∫£i
            time.sleep(0.5)

    except Exception as e:
        print(f"L·ªói t·ªïng th·ªÉ: {e}")

    print(f"\nT·ªïng s·ªë ·∫£nh thu ƒë∆∞·ª£c: {len(all_image_urls)}")
    return all_image_urls

def register_img1(bot):
    @bot.message_handler(commands=['img'])
    def handle_img(message):
        msg = bot.reply_to(message, "‚è≥ ƒêang x·ª≠ l√Ω... Vui l√≤ng ch·ªù!")

        image_urls = get_all_image_urls()

        try:
            if not image_urls:
                bot.send_message(message.chat.id, "‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh n√†o.")
                return

            # Ghi v√†o file
            with open("cosplay_links.txt", "w") as f:
                f.write("\n".join(image_urls))

            # G·ª≠i file
            bot.send_document(message.chat.id, InputFile("cosplay_links.txt"), caption=f"üì¶ T·ªïng c·ªông: {len(image_urls)} ·∫£nh")
        finally:
            bot.delete_message(msg.chat.id, msg.message_id)
