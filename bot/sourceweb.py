import os
import re
import requests
import zipfile
import tempfile
import urllib.parse
from bs4 import BeautifulSoup

def register_sourceweb(bot):
    @bot.message_handler(commands=['sourceweb'])
    def handle_sourceweb(message):
        args = message.text.split()
        if len(args) < 2:
            bot.reply_to(message, "Vui l√≤ng cung c·∫•p URL. V√≠ d·ª•: /sourceweb https://example.com")
            return

        url = args[1]
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url

        msg = bot.reply_to(message, "‚è≥ ƒêang x·ª≠ l√Ω... Vui l√≤ng ch·ªù!")

        try:
            domain = urllib.parse.urlparse(url).netloc
            zip_filename = f"{domain}_source.zip"

            with tempfile.TemporaryDirectory() as temp_dir:
                downloaded = download_website(url, temp_dir, bot, message, msg.message_id)

                if not downloaded:
                    bot.edit_message_text("üö´ Kh√¥ng th·ªÉ t·∫£i n·ªôi dung (l·ªói m·∫°ng ho·∫∑c v∆∞·ª£t qu√° 50MB).", message.chat.id, msg.message_id)
                    return

                zip_path = os.path.join(temp_dir, zip_filename)
                with zipfile.ZipFile(zip_path, 'w') as zipf:
                    for f in downloaded:
                        rel_path = os.path.relpath(f, temp_dir)
                        zipf.write(f, rel_path)

                with open(zip_path, 'rb') as f:
                    bot.send_document(message.chat.id, f, caption=f"üì¶ {len(downloaded)} file t·ª´ {url}")

            bot.delete_message(message.chat.id, msg.message_id)

        except Exception as e:
            bot.edit_message_text(f"‚ùå L·ªói: {str(e)}", message.chat.id, msg.message_id)

    def download_website(base_url, output_dir, bot, message, msg_id, max_total_size=50 * 1024 * 1024):
        processed = set()
        files = []
        queue = [base_url]
        domain = urllib.parse.urlparse(base_url).netloc
        headers = {'User-Agent': 'Mozilla/5.0'}
        total_size = 0

        while queue:
            url = queue.pop(0)
            if url in processed:
                continue
            processed.add(url)

            if urllib.parse.urlparse(url).netloc != domain:
                continue

            try:
                res = requests.get(url, headers=headers, timeout=10)
                if res.status_code != 200:
                    continue

                # X√°c ƒë·ªãnh lo·∫°i file v√† ƒë∆∞·ªùng d·∫´n l∆∞u
                ctype = res.headers.get('Content-Type', '').lower()
                path = urllib.parse.urlparse(url).path
                if not path or path.endswith('/'):
                    path += 'index.html'
                path = re.sub(r'[?#].*$', '', path)
                save_path = os.path.join(output_dir, domain, path.lstrip('/'))

                if save_path in files:
                    continue  # tr√°nh tr√πng file

                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                with open(save_path, 'wb') as f:
                    f.write(res.content)

                file_size = os.path.getsize(save_path)
                total_size += file_size
                if total_size > max_total_size:
                    break

                files.append(save_path)

                # N·∫øu l√† HTML th√¨ ph√¢n t√≠ch th√™m li√™n k·∫øt
                if 'text/html' in ctype:
                    soup = BeautifulSoup(res.text, 'html.parser')
                    for tag, attr in [('link', 'href'), ('script', 'src'), ('img', 'src')]:
                        for el in soup.find_all(tag):
                            src = el.get(attr)
                            if src:
                                full_url = urllib.parse.urljoin(url, src)
                                if urllib.parse.urlparse(full_url).netloc == domain and full_url not in processed:
                                    queue.append(full_url)

            except Exception as e:
                bot.edit_message_text(f"‚ö†Ô∏è L·ªói khi t·∫£i {url}:\n{e}", message.chat.id, msg_id)
                return []

        return files
